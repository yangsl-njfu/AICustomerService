# AIæ ¸å¿ƒä»£ç å¿«é€Ÿå‚è€ƒ

## ğŸ¯ æœ€é‡è¦çš„3ä¸ªæ–‡ä»¶

### 1. LangGraphå·¥ä½œæµ
**æ–‡ä»¶ï¼š** `backend/services/langgraph_workflow.py`

**æ ¸å¿ƒä»£ç ç‰‡æ®µï¼š**

```python
# çŠ¶æ€å®šä¹‰
class ConversationState(TypedDict):
    user_message: str
    intent: str
    response: str

# æ„å»ºå›¾
workflow = StateGraph(ConversationState)
workflow.add_node("intent", intent_node)
workflow.add_node("qa", qa_node)
workflow.add_conditional_edges("intent", route_decision)
graph = workflow.compile()

# æ‰§è¡Œ
result = await graph.ainvoke(initial_state)
```

### 2. å‘é‡æ£€ç´¢
**æ–‡ä»¶ï¼š** `backend/services/knowledge_retriever.py`

**æ ¸å¿ƒä»£ç ç‰‡æ®µï¼š**

```python
# åˆå§‹åŒ–
embeddings = OpenAIEmbeddings()
client = chromadb.Client()

# æ·»åŠ æ–‡æ¡£
embeddings_list = embeddings.embed_documents(texts)
collection.add(documents=texts, embeddings=embeddings_list)

# æ£€ç´¢
query_embedding = embeddings.embed_query(query)
results = collection.query(query_embeddings=[query_embedding], n_results=3)
```

### 3. æç¤ºè¯æ¨¡æ¿
**æ–‡ä»¶ï¼š** `backend/services/langgraph_workflow.py`

**æ ¸å¿ƒä»£ç ç‰‡æ®µï¼š**

```python
# æ„å»ºæç¤ºè¯
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯AIå®¢æœ..."),
    ("human", "ç”¨æˆ·é—®é¢˜ï¼š{question}")
])

# è°ƒç”¨LLM
response = await llm.ainvoke(prompt.format_messages(question="..."))
```

---

## ğŸ“ å¸¸ç”¨ä»£ç æ¨¡æ¿

### æ·»åŠ æ–°èŠ‚ç‚¹
```python
async def my_new_node(self, state: ConversationState):
    # 1. æ„å»ºæç¤ºè¯
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ çš„è§’è‰²å®šä¹‰"),
        ("human", "{input}")
    ])
    
    # 2. è°ƒç”¨LLM
    response = await self.llm.ainvoke(
        prompt.format_messages(input=state["user_message"])
    )
    
    # 3. æ›´æ–°çŠ¶æ€
    state["response"] = response.content
    
    return state
```

### æ·»åŠ æ¡ä»¶è·¯ç”±
```python
def my_route_decision(state: ConversationState) -> str:
    if state["confidence"] > 0.8:
        return "high_confidence_path"
    else:
        return "low_confidence_path"

# åœ¨å›¾ä¸­ä½¿ç”¨
workflow.add_conditional_edges(
    "source_node",
    my_route_decision,
    {
        "high_confidence_path": "node_a",
        "low_confidence_path": "node_b"
    }
)
```

### æ£€ç´¢å¹¶ç”Ÿæˆå›ç­”ï¼ˆRAGï¼‰
```python
# 1. æ£€ç´¢
docs = await knowledge_retriever.retrieve(query, top_k=3)

# 2. æ„å»ºä¸Šä¸‹æ–‡
context = "\n".join([doc.page_content for doc in docs])

# 3. ç”Ÿæˆå›ç­”
prompt = ChatPromptTemplate.from_messages([
    ("system", "åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å›ç­”"),
    ("human", "çŸ¥è¯†åº“ï¼š{context}\né—®é¢˜ï¼š{question}")
])

response = await llm.ainvoke(prompt.format_messages(
    context=context,
    question=query
))
```

---

## ğŸ”§ é…ç½®å‚æ•°

### LLMå‚æ•°ï¼ˆconfig.pyï¼‰
```python
OPENAI_MODEL = "gpt-4"           # æ¨¡å‹é€‰æ‹©
OPENAI_TEMPERATURE = 0.7         # åˆ›é€ æ€§ï¼ˆ0-2ï¼‰
OPENAI_MAX_TOKENS = 2000         # æœ€å¤§è¾“å‡ºé•¿åº¦
```

**æ¸©åº¦è¯´æ˜ï¼š**
- 0.0-0.3: ç¡®å®šæ€§å¼ºï¼Œé€‚åˆäº‹å®æ€§å›ç­”
- 0.4-0.7: å¹³è¡¡ï¼Œé€‚åˆå¯¹è¯
- 0.8-1.0: åˆ›é€ æ€§å¼ºï¼Œé€‚åˆåˆ›æ„å†…å®¹

### æ£€ç´¢å‚æ•°
```python
RETRIEVAL_TOP_K = 3              # æ£€ç´¢æ–‡æ¡£æ•°é‡
CONTEXT_MAX_HISTORY = 20         # ä¿ç•™å¯¹è¯è½®æ•°
```

---

## ğŸ› è°ƒè¯•å‘½ä»¤

### æµ‹è¯•å•ä¸ªç»„ä»¶
```python
# æµ‹è¯•æ„å›¾è¯†åˆ«
python -c "
from services.langgraph_workflow import langgraph_workflow
import asyncio

async def test():
    state = {'user_message': 'å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ'}
    result = await langgraph_workflow.intent_recognition_node(state)
    print(result)

asyncio.run(test())
"
```

### æŸ¥çœ‹å‘é‡åº“å†…å®¹
```python
# æŸ¥çœ‹é›†åˆç»Ÿè®¡
python -c "
from services.knowledge_retriever import knowledge_retriever
stats = knowledge_retriever.get_collection_stats()
print(stats)
"
```

---

## ğŸ’¡ å¿«é€Ÿä¿®æ”¹æŒ‡å—

### ä¿®æ”¹AIå›ç­”é£æ ¼
**ä½ç½®ï¼š** `langgraph_workflow.py` â†’ `qa_flow_node`

```python
# æ‰¾åˆ°è¿™è¡Œ
("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå®¢æœåŠ©æ‰‹...")

# ä¿®æ”¹ä¸º
("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½æ´»æ³¼çš„AIåŠ©æ‰‹ï¼Œç”¨è½»æ¾å¹½é»˜çš„è¯­æ°”å›ç­”...")
```

### æ·»åŠ æ–°çš„æ„å›¾ç±»å‹
**ä½ç½®ï¼š** `langgraph_workflow.py`

```python
# 1. åœ¨ intent_recognition_node çš„æç¤ºè¯ä¸­æ·»åŠ 
"- æŠ•è¯‰ï¼šç”¨æˆ·è¡¨è¾¾ä¸æ»¡æˆ–æŠ•è¯‰"

# 2. åœ¨ route_decision ä¸­æ·»åŠ è·¯ç”±
intent_map = {
    "é—®ç­”": "qa_flow",
    "å·¥å•": "ticket_flow",
    "æŠ•è¯‰": "complaint_flow"  # æ–°å¢
}

# 3. å®ç° complaint_flow_node
async def complaint_flow_node(self, state):
    # å¤„ç†æŠ•è¯‰é€»è¾‘
    pass

# 4. åœ¨ _build_graph ä¸­æ³¨å†Œ
workflow.add_node("complaint_flow", self.complaint_flow_node)
```

### è°ƒæ•´æ£€ç´¢æ•°é‡
**ä½ç½®ï¼š** `config.py`

```python
RETRIEVAL_TOP_K = 5  # ä»3æ”¹ä¸º5ï¼Œæ£€ç´¢æ›´å¤šæ–‡æ¡£
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### å‡å°‘LLMè°ƒç”¨æ¬¡æ•°
```python
# ç¼“å­˜å¸¸è§é—®é¢˜çš„å›ç­”
if state["user_message"] in common_questions:
    state["response"] = cached_answers[state["user_message"]]
    return state
```

### å¹¶è¡Œå¤„ç†
```python
import asyncio

# å¹¶è¡Œæ£€ç´¢å¤šä¸ªæ¥æº
results = await asyncio.gather(
    knowledge_retriever.retrieve(query, "knowledge_base"),
    knowledge_retriever.retrieve(query, "product_catalog")
)
```

---

## ğŸ“ å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£StateGraphçš„å·¥ä½œåŸç†
- [ ] èƒ½å¤Ÿæ·»åŠ æ–°çš„èŠ‚ç‚¹
- [ ] èƒ½å¤Ÿä¿®æ”¹æç¤ºè¯
- [ ] ç†è§£RAGæµç¨‹
- [ ] èƒ½å¤Ÿè°ƒè¯•LLMè¾“å‡º
- [ ] ç†è§£å‘é‡æ£€ç´¢åŸç†
- [ ] èƒ½å¤Ÿæ·»åŠ æ¡ä»¶è·¯ç”±
- [ ] ç†è§£ä¸Šä¸‹æ–‡ç®¡ç†

---

## ğŸ”— ç›¸å…³æ–‡ä»¶ç´¢å¼•

| åŠŸèƒ½ | æ–‡ä»¶è·¯å¾„ |
|------|----------|
| LangGraphå·¥ä½œæµ | `backend/services/langgraph_workflow.py` |
| å‘é‡æ£€ç´¢ | `backend/services/knowledge_retriever.py` |
| ä¸Šä¸‹æ–‡ç®¡ç† | `backend/services/redis_cache.py` |
| AIé…ç½® | `backend/config.py` |
| å¯¹è¯API | `backend/api/chat.py` |
| å®Œæ•´å­¦ä¹ æŒ‡å— | `AI_LEARNING_GUIDE.md` |
